{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "\n",
    "x = np.load('/Users/svenschnydrig/Documents/Coding Challenge/data/preprocessed/x_std.npy')\n",
    "y = np.load('/Users/svenschnydrig/Documents/Coding Challenge/data/preprocessed/y.npy')\n",
    "\n",
    "# Delete B1 (at index 0)\n",
    "x = np.delete(x, 18, axis=3)\n",
    "x = np.delete(x, 17, axis=3)\n",
    "x = np.delete(x, 16, axis=3)\n",
    "x = np.delete(x, 15, axis=3)\n",
    "x = np.delete(x, 14, axis=3)\n",
    "x = np.delete(x, 13, axis=3)\n",
    "x = np.delete(x, 12, axis=3)\n",
    "x = np.delete(x, 9, axis=3)\n",
    "x = np.delete(x, 0, axis=3)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.01, random_state=42)\n",
    "\n",
    "# Create a custom input layer for the 64x64x20 input\n",
    "input_layer = Input(shape=(64, 64, 11))\n",
    "\n",
    "# Load the ResNet50 model without the top classification layer and with custom input\n",
    "base_model = ResNet50(weights=None, include_top=False, input_tensor=input_layer)\n",
    "\n",
    "# Add a custom classification layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(2048, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.1))(x)  # additional fully-connected layer with L2 regularization with a factor of 0.01, 0.0001 to 0.1, start small\n",
    "x = Dropout(0.5)(x)  # dropout for regularization, 0.1-0.5, start small\n",
    "#x = Dropout(0.3)(x)  # dropout for regularization, 0.1-0.5, start small\n",
    "#x = Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # L2 regularization with a factor of 0.01, 0.0001 to 0.1, start small\n",
    "predictions = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "#model.compile(optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.SGD(learning_rate=0.003), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data augmentation generator\n",
    "datagen = ImageDataGenerator(rotation_range=20, shear_range=0.2,)\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='resnet50_wo_indices_more_reg_bigger.h5', monitor='val_loss', mode='auto', save_best_only=True, save_weights_only=False, verbose=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with the augmented data\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          steps_per_epoch=len(x_train) // batch_size,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs,\n",
    "          callbacks=[checkpoint_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "\n",
    "# Assuming your data is stored in x and y\n",
    "x = np.load('/Users/svenschnydrig/Documents/Coding Challenge/data/preprocessed/x_std.npy')\n",
    "y = np.load('/Users/svenschnydrig/Documents/Coding Challenge/data/preprocessed/y.npy')\n",
    "\n",
    "x_rgb = x[:,:,:, [3, 2, 1]].copy()\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_rgb, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom input layer for the 64x64x20 input\n",
    "input_layer = Input(shape=(64, 64, 3))\n",
    "\n",
    "# Add a resizing layer to resize the input to the size ResNet expects\n",
    "resizing_layer = Resizing(224, 224, interpolation=\"Bilinear\")(input_layer)\n",
    "\n",
    "# Load the ResNet101 model without the top classification layer and with custom input\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=resizing_layer)\n",
    "\n",
    "# Add a custom classification layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)  # additional fully-connected layer\n",
    "x = Dropout(0.2)(x)  # dropout for regularization, 0.1-0.5, start small\n",
    "x = Dense(1024, activation='relu')(x)  # additional fully-connected layer\n",
    "x = Dropout(0.2)(x)  # dropout for regularization, 0.1-0.5, start small\n",
    "#x = Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)  # L2 regularization with a factor of 0.01, 0.0001 to 0.1, start small\n",
    "\n",
    "predictions = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "# Freeze all layers in the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Unfreeze the last two residual blocks (9 layers)\n",
    "for layer in base_model.layers[-9:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.003), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    shear_range=0.2,  # added shear transformation\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    #horizontal_flip=True,\n",
    "    #vertical_flip=True,\n",
    "    #zoom_range=0.2,  # added zoom\n",
    ")\n",
    "\n",
    "# Define the checkpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='resnet50_std_wo_deeper_rgb.h5', monitor='val_accuracy', mode='max', save_best_only=True, save_weights_only=False, verbose=1)\n",
    "# Define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with the augmented data\n",
    "batch_size = 50\n",
    "epochs = 20\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          steps_per_epoch=len(x_train) // batch_size,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs,\n",
    "          callbacks=[checkpoint_callback, early_stopping_callback])  \n",
    "\n",
    "# Save the model\n",
    "#model.save('resnet50.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
